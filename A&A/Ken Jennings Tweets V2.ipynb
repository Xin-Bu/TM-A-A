{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ken Jennings Tweets\n",
    "### 74 Jeopardy Victories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tweepy\n",
    "\n",
    "# I've put my API keys in a .py file called API_keys.py\n",
    "from API_keys import api_key, api_key_secret, access_token, access_token_secret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Filecache to improve speed\n",
    "Setting a long timeout allowed for this to be run in multiple segments, picking up where it left off previously using the stored cached values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"./cache/\"\n",
    "timeout = 7200000\n",
    "\n",
    "cache = tweepy.FileCache(cache_dir,timeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Authenticate the Tweepy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True,cache=cache)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I'm putting the handles in a list to iterate through below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    @kenjennings has 26926 tweets. \n",
      "    That will take roughly 0 hours and 5.39 minutes\n",
      "    \n",
      "\n",
      "    @James_Holzhauer has 806 tweets. \n",
      "    That will take roughly 0 hours and 0.16 minutes\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "team_handles = ['kenjennings','James_Holzhauer']\n",
    "\n",
    "\n",
    "# This will iterate through each Twitter handle that we're collecting from\n",
    "for screen_name in team_handles:\n",
    "    \n",
    "    # Tells Tweepy we want information on the handle we're collecting from\n",
    "    # The next line specifies which information we want, which in this case is the number of status of users\n",
    "    user = api.get_user(screen_name) \n",
    "    statuses_count = user.statuses_count\n",
    "\n",
    "    # Let's see roughly how long it will take to grab all the Tweets\n",
    "    print(f'''\n",
    "    @{screen_name} has {statuses_count} tweets. \n",
    "    That will take roughly {statuses_count/(5000*60):.0f} hours and {statuses_count/(5000):.2f} minutes\n",
    "    ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.067339\n"
     ]
    }
   ],
   "source": [
    "# This creates a dictionary containing a list for each Twitter handle we'll be grabbing tweets from\n",
    "id_dict = {'kenjennings' : [], 'James_Holzhauer' : []}\n",
    "\n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# .keys() allows us to iterate through each key in the dictionary\n",
    "for handle in id_dict.keys():\n",
    "    \n",
    "    # To grab the tweets, we will be using followers_ids\n",
    "    for page in tweepy.Cursor(api.user_timeline,\n",
    "                             \n",
    "                              # Once the rate limit is hit, we will be notified that we must wait 15 mins (900 secs)\n",
    "                              wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True,\n",
    "                              tweet_mode=\"extended\",\n",
    "                              screen_name=handle).pages():\n",
    "\n",
    "        # The page variable comes back as a list, so we have to use .extend rather than .append\n",
    "        id_dict[handle].extend(page)\n",
    "        \n",
    "\n",
    "# Let's see how long it took to grab all Tweets\n",
    "end_time = datetime.datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['id','full_text']\n",
    "\n",
    "for account in id_dict.keys():\n",
    "     # Descriptions with emoji or non-Roman letters can cause trouble. Encoding your .txt file in utf-8 will help\n",
    "    with open(f'{account}_tweets.txt','w', encoding='utf-8') as out_file:\n",
    "        out_file.write('\\t'.join(headers) + '\\n')\n",
    "\n",
    "        for idx, ids in enumerate(id_dict[account]):\n",
    "\n",
    "            outline = [ids.id,\n",
    "                       ids.full_text.replace(\"\\n\",\"\")]\n",
    "\n",
    "            out_file.write('\\t'.join([f\"{str(item):<15}\" for item in outline]) + '\\n')\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* screen_name\n",
    "* name\n",
    "* id\n",
    "* location\n",
    "* followers_count\n",
    "* friends_count\n",
    "* description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw=stopwords.words('english')\n",
    "\n",
    "sw.append('rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_patterns(text):\n",
    "        text_clean=[w for w in text.lower().split()]\n",
    "        text_clean=[w for w in text_clean if w not in sw]\n",
    "        text_clean=[w for w in text_clean if w.isalpha()]\n",
    "        \n",
    "        total_tokens=len(text_clean)\n",
    "        unique_tokens=len(set(text_clean))\n",
    "        \n",
    "        text_clean_len=[len(w) for w in text_clean]\n",
    "        \n",
    "        lex_diversity=len(set(text_clean))/len(text_clean)\n",
    "        \n",
    "        top_20=Counter(text_clean).most_common(20)\n",
    "        \n",
    "        results={'tokens':total_tokens,\n",
    "                'unique tokens':unique_tokens,\n",
    "                'average token length':lex_diversity,\n",
    "                'top 20':top_20}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(text):\n",
    "        text = text.lower().replace(\":\",\"\").replace(\",\",\"\").replace(\"!\",\"\")\n",
    "        usernames=[u for u in text.split()]\n",
    "       \n",
    "    #I am looking for the usernames coming with @\n",
    "        usernames=[u for u in usernames if u[0] == '@']\n",
    "        \n",
    "        total_tokens=len(usernames)\n",
    "        unique_tokens=len(set(usernames))\n",
    "\n",
    "        top_30=Counter(usernames).most_common(30)\n",
    "        \n",
    "        results={'user mentions':total_tokens,\n",
    "                'unique user mentions':unique_tokens,\n",
    "             \n",
    "                'top 30':top_30}\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kenjennings\n",
      "{'tokens': 17742, 'unique tokens': 5764, 'average token length': 0.32487881862247775, 'top 20': [('new', 243), ('like', 231), ('one', 134), ('get', 100), ('good', 93), ('time', 87), ('people', 86), ('know', 85), ('think', 85), ('trivia', 79), ('see', 78), ('going', 74), ('first', 68), ('never', 68), ('would', 67), ('every', 66), ('jeopardy', 63), ('last', 62), ('really', 61), ('love', 59)]}\n",
      "\n",
      "{'user mentions': 2661, 'unique user mentions': 1105, 'top 30': [('@omnibusproject', 197), ('@johnroderick', 171), ('@kenjennings', 54), ('@jeopardy', 50), ('@mental_floss', 45), ('@gameshownetwork', 44), ('@buzztronics', 41), ('@james_holzhauer', 34), ('@jessethorn', 30), ('@billcorbett', 28), ('@andyrichter', 24), ('@paulandstorm', 22), ('@omnibusproject.', 22), ('@bradrutter', 21), ('@andylevy', 21), ('@desijed', 19), ('@petersagal', 19), ('@muffymarracco', 18), ('@mrtimlong', 16), ('@blainecapatch', 15), ('@1followernodad', 14), ('@louisvirtel', 14), ('@apelad', 13), ('@jamesurbaniak', 13), ('@mkupperman', 13), ('@pixelatedboat', 13), ('@mailchimp', 13), ('@austintylerro', 12), ('@hughhowey', 11), ('@thethomason', 11)]}\n",
      "----\n",
      "James_Holzhauer\n",
      "{'tokens': 4562, 'unique tokens': 2010, 'average token length': 0.44059622972380535, 'top 20': [('james', 58), ('jeopardy', 51), ('one', 34), ('get', 34), ('like', 30), ('alex', 27), ('win', 27), ('time', 26), ('game', 26), ('holzhauer', 25), ('first', 24), ('never', 22), ('good', 21), ('two', 21), ('greatest', 20), ('show', 20), ('got', 20), ('ken', 20), ('see', 19), ('could', 18)]}\n",
      "\n",
      "{'user mentions': 448, 'unique user mentions': 210, 'top 30': [('@jeopardy', 70), ('@james_holzhauer', 46), ('@kenjennings', 34), ('@bradrutter', 26), ('@abcnetwork', 9), ('@goldenknights', 7), ('@bomani_jones', 6), ('@buzztronics', 5), ('@patmcafeeshow', 5), ('@gma', 5), ('@tweetophobicpam', 3), ('@kenjenningsâ€™s', 3), ('@ringer', 3), ('@project150lv', 3), ('@whoisalexjacob', 3), ('@normmacdonald', 3), ('@8newsnow', 3), ('@zbeg', 3), ('@clairemcnear', 2), ('@rogcraig', 2), ('@acblbridge', 2), ('@ktnv', 2), ('@mike_partypoker', 2), (\"@jeopardy's\", 2), ('@consumer_cell.', 2), ('@drake', 2), ('@cubs', 2), ('@peter_king', 2), ('@davidpurdum', 2), ('@tombrady', 2)]}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#Go through all the tweets from the dictionary i generated and combine the full text for analysis functions\n",
    "for account in id_dict.keys():\n",
    "    full_text = \"\"\n",
    "\n",
    "    for idx, ids in enumerate(id_dict[account]):\n",
    "        full_text = full_text + \" \" + (ids.full_text.replace(\"\\n\",\"\"))\n",
    "\n",
    "    print(account)\n",
    "    print(get_patterns(full_text))\n",
    "    print()\n",
    "    print(get_users(full_text))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One question I would like to answer is: What insights can we gain from their usernames? \n",
    "We gain insights from the most thirty common usernames. Generally, the two rivals interact frequently with each other. For example, @james_holzhauer showed up in Ken's account for 34 times and vise versa. Both of them interacted with @jeopardy very often: James interacted with @jeopardy 70 times compared with 50 times from Ken. To Ken, @omnibusproject is a very important project co-run by Ken and John Roderick. Therefore, we can see @omnibusproject and @johnroderick are the top two most common user mentions. In comparison, James seems to have more attention on Jeopardy because Jeopardy is the top one user mentions. Also Jeames may be a fan of Las Vegas hockey team because the username @goldenknights was mentioned seven times. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
